
@online{29,
	title = {{ravinduJayUoM}/{enhanceLetterWritingSinhala}: Enhance Letter Writing Sinhala},
	url = {https://github.com/ravinduJayUoM/enhanceLetterWritingSinhala},
	urldate = {2025-05-26},
}

@online{28,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	url = {https://www.researchgate.net/publication/341639856_Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks},
	urldate = {2025-05-26},
}

@online{27,
	title = {Adapter-based fine-tuning of pre-trained multilingual language models for code-mixed and code-switched text classification {\textbar} Request {PDF}},
	url = {https://www.researchgate.net/publication/361708194_Adapter-based_fine-tuning_of_pre-trained_multilingual_language_models_for_code-mixed_and_code-switched_text_classification},
	urldate = {2025-05-26},
}

@online{26,
	title = {{LoRA}: Low-Rank Adaptation of Large Language Models {\textbar} Request {PDF}},
	url = {https://www.researchgate.net/publication/352504883_LoRA_Low-Rank_Adaptation_of_Large_Language_Models},
	urldate = {2025-05-26},
}

@inproceedings{25,
	title = {Parameter-Efficient Transfer Learning for {NLP}},
	url = {https://proceedings.mlr.press/v97/houlsby19a.html},
	abstract = {Fine-tuning large pretrained models is an effective transfer mechanism in {NLP}. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed {BERT} Transformer model to \$26\$ diverse text classification tasks, including the {GLUE} benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On {GLUE}, we attain within \$0.8\%\$ of the performance of full fine-tuning, adding only \$3.6\%\$ parameters per task. By contrast, fine-tuning trains \$100\%\$ of the parameters per task.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2790--2799},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin De and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	urldate = {2025-05-26},
	date = {2019-05-24},
	langid = {english},
}

@online{24,
	title = {{InstructAlign}: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning - {ACL} Anthology},
	url = {https://aclanthology.org/2023.sealp-1.5/},
	urldate = {2025-05-26},
}

@online{23,
	title = {({PDF}) Prompt Engineering or Fine-Tuning? A Case Study on Phishing Detection with Large Language Models},
	url = {https://www.researchgate.net/publication/378055315_Prompt_Engineering_or_Fine-Tuning_A_Case_Study_on_Phishing_Detection_with_Large_Language_Models},
	urldate = {2025-05-26},
}

@online{22,
	title = {({PDF}) Data Augmentation to Address Out-of-Vocabulary Problem in Low-Resource Sinhala-English Neural Machine Translation},
	url = {https://www.researchgate.net/publication/360698226_Data_Augmentation_to_Address_Out-of-Vocabulary_Problem_in_Low-Resource_Sinhala-English_Neural_Machine_Translation},
	abstract = {{PDF} {\textbar} Out-of-Vocabulary ({OOV}) is a problem for Neural Machine Translation ({NMT}). {OOV} refers to words with a low occurrence in the training data, or to... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	titleaddon = {{ResearchGate}},
	urldate = {2025-05-26},
	langid = {english},
	doi = {10.48550/arXiv.2205.08722},
}

@inproceedings{21,
	location = {Berlin, Germany},
	title = {Neural Machine Translation of Rare Words with Subword Units},
	url = {https://aclanthology.org/P16-1162/},
	doi = {10.18653/v1/P16-1162},
	eventtitle = {{ACL} 2016},
	pages = {1715--1725},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	editor = {Erk, Katrin and Smith, Noah A.},
	urldate = {2025-05-26},
	date = {2016-08},
}

@misc{20,
	title = {Multilingual {LLMs} are Better Cross-lingual In-context Learners with Alignment},
	url = {http://arxiv.org/abs/2305.05940},
	doi = {10.48550/arXiv.2305.05940},
	abstract = {In-context learning ({ICL}) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. {ICL}-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored {ICL} in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of {ICL} for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual {ICL}, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy -- Cross-lingual In-context Source-Target Alignment (X-{InSTA}). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-{InSTA} is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.},
	number = {{arXiv}:2305.05940},
	publisher = {{arXiv}},
	author = {Tanwar, Eshaan and Dutta, Subhabrata and Borthakur, Manish and Chakraborty, Tanmoy},
	urldate = {2025-05-26},
	date = {2023-06-24},
	eprinttype = {arxiv},
	eprint = {2305.05940 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@online{19,
	title = {Meta {AI} Research Topic - No Language Left Behind},
	url = {https://ai.meta.com/research/no-language-left-behind/},
	urldate = {2025-05-26},
}

@online{18,
	title = {({PDF}) Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task},
	url = {https://www.researchgate.net/publication/390545745_Multilingual_Retrieval-Augmented_Generation_for_Knowledge-Intensive_Task},
	abstract = {{PDF} {\textbar} Retrieval-augmented generation ({RAG}) has become a cornerstone of contemporary {NLP}, enhancing large language models ({LLMs}) by allowing them to... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	titleaddon = {{ResearchGate}},
	urldate = {2025-05-26},
	date = {2025-04-09},
	langid = {english},
	doi = {10.48550/arXiv.2504.03616},
}

@inproceedings{17,
	location = {Mexico City, Mexico},
	title = {{LLMs} Are Few-Shot In-Context Low-Resource Language Learners},
	url = {https://aclanthology.org/2024.naacl-long.24/},
	doi = {10.18653/v1/2024.naacl-long.24},
	abstract = {In-context learning ({ICL}) empowers large language models ({LLMs}) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages.Nonetheless, there is only a handful of works explored {ICL} for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study {ICL} and its cross-lingual variation (X-{ICL}) on 25 low-resource and 7 relatively higher-resource languages.Our study not only assesses the effectiveness of {ICL} with {LLMs} in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of {ICL} for low-resource languages.Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of {LLMs} through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing {ICL} research, particularly for low-resource languages.},
	eventtitle = {{NAACL}-{HLT} 2024},
	pages = {405--433},
	booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Cahyawijaya, Samuel and Lovenia, Holy and Fung, Pascale},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	urldate = {2025-05-26},
	date = {2024-06},
}

@inproceedings{16,
	location = {Florence, Italy},
	title = {Choosing Transfer Languages for Cross-Lingual Learning},
	url = {https://aclanthology.org/P19-1301/},
	doi = {10.18653/v1/P19-1301},
	abstract = {Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing ({NLP}) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative {NLP} tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different {NLP} tasks, which may inform future ad hoc selection even without use of our method.},
	eventtitle = {{ACL} 2019},
	pages = {3125--3135},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Yu-Hsiang and Chen, Chian-Yu and Lee, Jean and Li, Zirui and Zhang, Yuyan and Xia, Mengzhou and Rijhwani, Shruti and He, Junxian and Zhang, Zhisong and Ma, Xuezhe and Anastasopoulos, Antonios and Littell, Patrick and Neubig, Graham},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	urldate = {2025-05-26},
	date = {2019-07},
}

@online{14,
	title = {{NLPC}-{UOM}/{SinBERT}-large · Hugging Face},
	url = {https://huggingface.co/NLPC-UOM/SinBERT-large},
	urldate = {2025-05-26},
}

@online{13,
	title = {Papers with Code - {CC}100 Dataset},
	url = {https://paperswithcode.com/dataset/cc100},
	abstract = {This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages. This was constructed using the urls and paragraph indices provided by the {CC}-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source {CC}-Net repository.},
	urldate = {2025-05-26},
	langid = {english},
}

@inproceedings{12,
	location = {Dubrovnik, Croatia},
	title = {Large Scale Multi-Lingual Multi-Modal Summarization Dataset},
	url = {https://aclanthology.org/2023.eacl-main.263/},
	doi = {10.18653/v1/2023.eacl-main.263},
	abstract = {Significant developments in techniques such as encoder-decoder models have enabled us to represent information comprising multiple modalities. This information can further enhance many downstream tasks in the field of information retrieval and natural language processing; however, improvements in multi-modal techniques and their performance evaluation require large-scale multi-modal data which offers sufficient diversity. Multi-lingual modeling for a variety of tasks like multi-modal summarization, text generation, and translation leverages information derived from high-quality multi-lingual annotated data. In this work, we present the current largest multi-lingual multi-modal summarization dataset (M3LS), and it consists of over a million instances of document-image pairs along with a professionally annotated multi-modal summary for each pair. It is derived from news articles published by British Broadcasting Corporation({BBC}) over a decade and spans 20 languages, targeting diversity across five language roots, it is also the largest summarization dataset for 13 languages and consists of cross-lingual summarization data for 2 languages. We formally define the multi-lingual multi-modal summarization task utilizing our dataset and report baseline scores from various state-of-the-art summarization techniques in a multi-lingual setting. We also compare it with many similar datasets to analyze the uniqueness and difficulty of M3LS. The dataset and code used in this work are made available at “https://github.com/anubhav-jangra/M3LS”.},
	eventtitle = {{EACL} 2023},
	pages = {3620--3632},
	booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Verma, Yash and Jangra, Anubhav and Verma, Raghvendra and Saha, Sriparna},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	urldate = {2025-05-26},
	date = {2023-05},
}

@inproceedings{11,
	location = {Online},
	title = {{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages},
	url = {https://aclanthology.org/2021.findings-acl.413/},
	doi = {10.18653/v1/2021.findings-acl.413},
	shorttitle = {{XL}-Sum},
	eventtitle = {Findings 2021},
	pages = {4693--4703},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md. Saiful and Mubasshir, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M. Sohel and Shahriyar, Rifat},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	urldate = {2025-05-26},
	date = {2021-08},
}

@online{10,
	title = {"Talking Books" : A Sinhala Abstractive Text Summarization Approach for Sinhala Textbooks {\textbar} {IEEE} Conference Publication {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/document/10126205},
	urldate = {2025-05-26},
}

@inproceedings{8,
	location = {Florence, Italy},
	title = {The {BEA}-2019 Shared Task on Grammatical Error Correction},
	url = {https://aclanthology.org/W19-4406/},
	doi = {10.18653/v1/W19-4406},
	abstract = {This paper reports on the {BEA}-2019 Shared Task on Grammatical Error Correction ({GEC}). As with the {CoNLL}-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the {BEA}-2019 shared task is the introduction of a new dataset, the Write\&Improve+{LOCNESS} corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of tracks, which control the amount of annotated data available to participants. Systems are evaluated in terms of {ERRANT} F\_0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set.},
	eventtitle = {{BEA} 2019},
	pages = {52--75},
	booktitle = {Proceedings of the Fourteenth Workshop on Innovative Use of {NLP} for Building Educational Applications},
	publisher = {Association for Computational Linguistics},
	author = {Bryant, Christopher and Felice, Mariano and Andersen, Øistein E. and Briscoe, Ted},
	editor = {Yannakoudakis, Helen and Kochmar, Ekaterina and Leacock, Claudia and Madnani, Nitin and Pilán, Ildikó and Zesch, Torsten},
	urldate = {2025-05-26},
	date = {2019-08},
}

@inproceedings{7,
	location = {Baltimore, Maryland},
	title = {The {CoNLL}-2014 Shared Task on Grammatical Error Correction},
	url = {https://aclanthology.org/W14-1701/},
	doi = {10.3115/v1/W14-1701},
	eventtitle = {{CoNLL} 2014},
	pages = {1--14},
	booktitle = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task},
	publisher = {Association for Computational Linguistics},
	author = {Ng, Hwee Tou and Wu, Siew Mei and Briscoe, Ted and Hadiwinoto, Christian and Susanto, Raymond Hendy and Bryant, Christopher},
	editor = {Ng, Hwee Tou and Wu, Siew Mei and Briscoe, Ted and Hadiwinoto, Christian and Susanto, Raymond Hendy and Bryant, Christopher},
	urldate = {2025-05-26},
	date = {2014-06},
}

@online{5,
	title = {Grammar Error Correction for Less Resourceful Languages: A Case Study of Sinhala {\textbar} {IEEE} Conference Publication {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/abstract/document/10253578},
	urldate = {2025-05-26},
}

@inproceedings{6,
	title = {Grammar Error Correction for Less Resourceful Languages: A Case Study of Sinhala},
	url = {https://ieeexplore.ieee.org/document/10253578},
	doi = {10.1109/ICIIS58898.2023.10253578},
	shorttitle = {Grammar Error Correction for Less Resourceful Languages},
	abstract = {Grammatical Error Correction ({GEC}) is crucial for improving the readability and comprehension of text. Although substantial advancements have been achieved in this area for widely-spoken languages such as English, the focus on the development of {GEC} tools for less common languages such as Sinhala has been inadequate. Sinhala is a language spoken by more than 16 million people in Sri Lanka, known for its rich morphology, and complex grammar structures that pose a challenge for Sinhala {GEC} systems. This paper presents a novel {GEC} approach that utilizes Google machine translation, cross-linguistic knowledge and rule-based techniques augmented by machine learning to analyze complex Sinhala sentences. We focus on analyzing Sinhala verb agreement rules, and object validation rules in Sinhala active voice sentences. Additionally, we address the major challenges in Sinhala {GEC}, such as subject and object detection and the detection of grammatical features of nouns, including animacy, gender, and number. Our findings indicate that the {GEC} methodology presented achieved an accuracy of 75.61 \%. Additionally, the gender and number detection components produced an accuracy of 90.89\% and 92.33\%, respectively. These results demonstrate the effectiveness of our approach in identifying and correcting errors in complex Sinhala sentences. Our approach is particularly useful in languages with rich morphology and limited annotated data.},
	eventtitle = {2023 {IEEE} 17th International Conference on Industrial and Information Systems ({ICIIS})},
	pages = {169--174},
	booktitle = {2023 {IEEE} 17th International Conference on Industrial and Information Systems ({ICIIS})},
	author = {Jayasuriya, Pradeep and Wijesundara, Malitha and Thelijjagoda, Samantha and Kodagoda, Nuwan},
	urldate = {2025-05-26},
	date = {2023-08},
	note = {{ISSN}: 2164-7011},
	keywords = {Error correction, Feature extraction, Grammar, Grammar Error Analysis, Grammatical feature detection, Internet, Machine Learning, Machine learning, Morphology, Natural Language Processing, Object detection, Sinhala},
}

@online{4,
	title = {{SinMorphy}: A Morphological Analyzer for the Sinhala Language {\textbar} {IEEE} Conference Publication {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/document/9525636},
	urldate = {2025-05-26},
}

@online{3,
	title = {Defining the Gold Standard Definitions for the Morphology of Sinhala Words {\textbar} Request {PDF}},
	url = {https://www.researchgate.net/publication/339207668_Defining_the_Gold_Standard_Definitions_for_the_Morphology_of_Sinhala_Words},
	urldate = {2025-05-26},
}

@inproceedings{2,
	location = {Marseille, France},
	title = {{BERTifying} Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},
	url = {https://aclanthology.org/2022.lrec-1.803/},
	abstract = {This research provides the first comprehensive analysis of the performance of pre-trained language models for Sinhala text classification. We test on a set of different Sinhala text classification tasks and our analysis shows that out of the pre-trained multilingual models that include Sinhala ({XLM}-R, {LaBSE}, and {LASER}), {XLM}-R is the best model by far for Sinhala text classification. We also pre-train two {RoBERTa}-based monolingual Sinhala models, which are far superior to the existing pre-trained language models for Sinhala. We show that when fine-tuned, these pre-trained language models set a very strong baseline for Sinhala text classification and are robust in situations where labeled data is insufficient for fine-tuning. We further provide a set of recommendations for using pre-trained models for Sinhala text classification. We also introduce new annotated datasets useful for future research in Sinhala text classification and publicly release our pre-trained models.},
	eventtitle = {{LREC} 2022},
	pages = {7377--7385},
	booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Dhananjaya, Vinura and Demotte, Piyumal and Ranathunga, Surangika and Jayasena, Sanath},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	urldate = {2025-05-26},
	date = {2022-06},
}

@online{1,
	title = {({PDF}) Survey on Publicly Available Sinhala Natural Language Processing Tools and Research},
	url = {https://www.researchgate.net/publication/333649787_Survey_on_Publicly_Available_Sinhala_Natural_Language_Processing_Tools_and_Research},
	abstract = {{PDF} {\textbar} Sinhala is the native language of the Sinhalese people who make up the largest ethnic group of Sri Lanka. The language belongs to the... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	titleaddon = {{ResearchGate}},
	urldate = {2025-05-26},
	langid = {english},
	doi = {10.48550/arXiv.1906.02358},
}

@inproceedings{15,
	location = {Abu Dhabi},
	title = {Sinhala Transliteration: A Comparative Analysis Between Rule-based and Seq2Seq Approaches},
	url = {https://aclanthology.org/2025.indonlp-1.19/},
	shorttitle = {Sinhala Transliteration},
	abstract = {Due to reasons of convenience and lack of tech literacy, transliteration (i.e., Romanizing native scripts instead of using localization tools) is eminently prevalent in the context of low-resource languages such as Sinhala, which have their own writing script. In this study, our focus is on Romanized Sinhala transliteration. We propose two methods to address this problem: Our baseline is a rule-based method, which is then compared against our second method where we approach the transliteration problem as a sequence-to-sequence task akin to the established Neural Machine Translation ({NMT}) task. For the latter, we propose a Transformer based Encode-Decoder solution. We witnessed that the Transformer-based method could grab many ad-hoc patterns within the Romanized scripts compared to the rule-based method.},
	pages = {166--173},
	booktitle = {Proceedings of the First Workshop on Natural Language Processing for Indo-Aryan and Dravidian Languages},
	publisher = {Association for Computational Linguistics},
	author = {De Mel, Yomal and Wickramasinghe, Kasun and de Silva, Nisansa and Ranathunga, Surangika},
	editor = {Weerasinghe, Ruvan and Anuradha, Isuri and Sumanathilaka, Deshan},
	urldate = {2025-05-26},
	date = {2025-01},
}
